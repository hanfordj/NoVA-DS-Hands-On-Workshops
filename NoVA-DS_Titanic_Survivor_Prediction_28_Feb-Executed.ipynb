{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 101 Hands On Workshop - Session #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Recap:\n",
    "\n",
    "In the last workshop we went through:\n",
    "* Initial Exploratory Data Analysis of the Titanic Dataset.\n",
    "* Built some simple models (Random Forest, SVM).\n",
    "* Used GridSearchCV to optimize hyperparameters (quick intro).\n",
    "\n",
    "Takeaways:\n",
    "* Overall, top performance ~ **83% accuracy** (on holdout set), **77.03% accuracy** on Kaggle public holdout using SVC and GridSearch.\n",
    "* Most of the time and discussion revolved around data analysis and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda for this Session:  <a class=\"anchor\" id=\"agenda\"></a>\n",
    "* [Utility functions](#utility-functions)\n",
    "* [Preparation of data](#prep-of-data)\n",
    "* [Using Scikit-Learn Pipelines to simplify and modularize the data pipeline](#sklearn-pipelines)\n",
    "  - [Exercise 1](#exercise-1)\n",
    "  - [Simple Pipeline](#simple-pipeline)\n",
    "  - [More Complex Pipeline](#complex-pipeline)\n",
    "* [Using Grid Search / Randomized Search to optimize hyperparameters](#search-for-hyperparameters)\n",
    "  - [Exercise 2](#exercise-2)\n",
    "* [Understanding which results are overfitting / underfitting](#overfitting-underfitting)\n",
    "* [Write Kaggle Submission file based on predicted outputs](#kaggle-submission)\n",
    "  - [Exercise 3 - Homework](#exercise-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 28629 Mar  2 21:23 test.csv\n",
      "-rw-r--r-- 1 root root 61194 Mar  2 21:23 train.csv\n",
      "  892 train.csv\n",
      "  419 test.csv\n",
      " 1311 total\n"
     ]
    }
   ],
   "source": [
    "# Get the data:\n",
    "#  This will pull the required data (train.csv and test.csv files) from dropbox.\n",
    "#  The cool thing is that this is leveraging wget on the docker container to pull data to your local disk,\n",
    "#  consistently for all operating systems.\n",
    "! rm -f train.csv test.csv\n",
    "! wget https://www.dropbox.com/s/f7fb3gon8byyyz6/train.csv?dl=1 -O train.csv -q\n",
    "! wget https://www.dropbox.com/s/zcd2751x6waex9f/test.csv?dl=1 -O test.csv -q\n",
    "! ls -l train.csv test.csv\n",
    "! wc -l train.csv test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main Python libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import imp\n",
    "import collections\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, ElasticNet\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, matthews_corrcoef\n",
    "from sklearn.base import TransformerMixin, BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions <a class=\"anchor\" id=\"utility-functions\"></a>\n",
    "(Back to [agenda/toc](#agenda))\n",
    "\n",
    "* Functions to use in pipelines - from Session #1\n",
    "  - model_eval\n",
    "  - fare_imputer\n",
    "  - embarked_imputer\n",
    "  - transformer_dummies\n",
    "  - select_numeric_columns\n",
    "  - columns_subset\n",
    "  - custom_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(y_actual, y_pred, label=\"Model\"):\n",
    "    \"\"\"\n",
    "    Provides classification evaluation metrics based on actual's and predicted values; namely:\n",
    "    * Accuracy\n",
    "    * F1 Score\n",
    "    * Matthew's Correlation Coefficient\n",
    "    * Confusion Matrix\n",
    "    \"\"\"\n",
    "    logging.info(\"{} - Accuracy: {}\".format(label, accuracy_score(y_actual, y_pred)))\n",
    "    logging.info(\"{} - F1 Score: {}\".format(label, f1_score(y_actual, y_pred)))\n",
    "    logging.info(\"{} - Matthew's Corr Coef: {}\".format(label, matthews_corrcoef(y_actual, y_pred)))\n",
    "    return pd.DataFrame(confusion_matrix(y_actual, y_pred),\n",
    "                        columns=['Pred_Died','Pred_Survived'],\n",
    "                        index=['Actual_Died','Actual_Survived'])\n",
    "\n",
    "def fare_imputer(X_):\n",
    "    \"\"\"\n",
    "    Designed to be passed to a FunctionTransformer with a pd.DataFrame as input, with validate=False.\n",
    "    * Based on Session #1 logic:\n",
    "      - Impute Fare values for records where Fare is null or 0.\n",
    "      - Impute values by calculating the median Fare by Pclass and then using this median value for imputation.\n",
    "    \"\"\"\n",
    "    X = X_.copy()\n",
    "    # Fare Imputer:\n",
    "    pclass_to_fare_lookup = X[~((X.Fare == 0) | (X.Fare.isnull())) ].groupby(['Pclass']).Fare.median()\n",
    "    X.loc[((X.Fare == 0) | (X.Fare.isnull())), 'Fare'] = X['Pclass'].map(dict(pclass_to_fare_lookup))\n",
    "    return X\n",
    "\n",
    "def embarked_imputer(X_):\n",
    "    \"\"\"\n",
    "    Designed to be passed to a FunctionTransformer with a pd.DataFrame as input, with validate=False.\n",
    "    * Based on Session #1 logic:\n",
    "      - Based on Fare level of missing values, set all nulls to 'S'.\n",
    "    \"\"\"\n",
    "    X = X_.copy()\n",
    "    #Embarked Imputer:\n",
    "    X.Embarked.fillna('S',inplace=True)\n",
    "    return X\n",
    "\n",
    "def transformer_dummies(X_, columns=None, transform_to_cat=True):\n",
    "    \"\"\"\n",
    "    Designed to be passed to a FunctionTransformer with a pd.DataFrame as input, with validate=False.\n",
    "    * Leverages Pandas get_dummies to one-hot encode categorical columns.\n",
    "    * Key here is that columns to be encoded should ideally be \"Category\" columns so that get_dummies generates\n",
    "      the same number of columns in the same order for different cuts of the data.\n",
    "    \"\"\"\n",
    "    X = X_.copy()\n",
    "    if transform_to_cat:\n",
    "        for c in columns:\n",
    "            X[c] = pd.Categorical(X[c])\n",
    "    \n",
    "    return pd.get_dummies(X, drop_first=True, dummy_na=True, columns=columns)\n",
    "\n",
    "def select_numeric_cols(X):\n",
    "    \"\"\"\n",
    "    Designed to be passed to a FunctionTransformer with a pd.DataFrame as input, with validate=False.\n",
    "    This function will subset the selection to only numeric columns.\n",
    "    ColumnExtractor class provides a better, more flexible implementation of this logic. \n",
    "    \"\"\"\n",
    "    return X.select_dtypes([np.int64,np.float64,np.uint8])\n",
    "\n",
    "def columns_subset(X, columns):\n",
    "    \"\"\"\n",
    "    Designed to be passed to a FunctionTransformer with a pd.DataFrame as input, with validate=False.\n",
    "    Need to pass columns in as a kw_args parameter; e.g.\n",
    "    FunctionTransformer( columns_subset, validate=False, kw_args = { 'columns' : ['A', 'B', ... ] } )\n",
    "    \"\"\"\n",
    "    return X[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transformer(X_):\n",
    "    \"\"\"\n",
    "    Designed to be passed to a FunctionTransformer with a pd.DataFrame as input, with validate=False.\n",
    "    Performs feature creation steps as discussed in Session #1.\n",
    "    \"\"\"\n",
    "    X = X_.copy()\n",
    "        \n",
    "    X['fare_log'] = np.log(X.Fare)\n",
    "\n",
    "    # Age:\n",
    "    X['age_16_to_34'] = ((X.Age >= 16) & (X.Age <= 34)).apply(int)\n",
    "    X['age_35_to_47'] = ((X.Age >= 35) & (X.Age <= 47)).apply(int)\n",
    "    X['age_missing'] = (X.Age.isnull()).apply(int)\n",
    "    X.drop(['Age'], axis=1, inplace=True)\n",
    "    \n",
    "    # People per ticket:\n",
    "    X = X.merge(pd.DataFrame(X.groupby('Ticket').size(),\n",
    "                             columns=['people_per_ticket']),\n",
    "                how='left', left_on='Ticket', right_index=True)\n",
    "    X.drop(['Ticket'], axis=1, inplace=True)\n",
    "    \n",
    "    # People per lastname:\n",
    "    X['Lastname'] = X.Name.apply(lambda x: x.split(',')[0])\n",
    "    X = X.merge(pd.DataFrame(X.groupby('Lastname').size(),\n",
    "                             columns=['people_per_lastname']),\n",
    "                how='left', left_on='Lastname', right_index=True)\n",
    "    X.drop(['Name', 'Lastname'], axis=1, inplace=True)\n",
    "    \n",
    "    # People in family:\n",
    "    X['people_in_family'] = X.apply(lambda x: x.Parch+x.SibSp+1,axis=1)\n",
    "    X.drop(['Parch','SibSp'],axis=1,inplace=True)\n",
    "    \n",
    "    # Cabin information extraction:\n",
    "    X.Cabin.fillna('O',inplace=True) # O = other\n",
    "    #X['cabin_level'] = X.Cabin.apply(lambda x: x[0])\n",
    "    X['cabin_level'] = pd.Categorical(X.Cabin.apply(lambda x: x[0]),\n",
    "                                      categories=['O', 'C', 'E', 'G', 'D', 'A', 'B', 'F', 'T'])\n",
    "    X.drop(['Cabin'], axis=1, inplace=True)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit zacstewart.com (http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html)\n",
    "class ModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom estimator which fits the passed model on train dataset; transform returns model predictions.\n",
    "    Convenience function to add model predictions as features in a pipeline.\n",
    "    Credit: zacstewart.com\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(self.model.predict(X))\n",
    "\n",
    "\n",
    "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom estimator which extracts subset of columns from a Panda's dataframe based on:\n",
    "    * Columns list, or\n",
    "    * Dtypes list\n",
    "    Supports returning data as a DataFrame or a Numpy Array; based on 'np_return' parameter.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None, dtypes=None, np_return=True):\n",
    "        if columns is None and dtypes is None:\n",
    "            raise ValueError(\"Need to specify 'columns' or 'dtypes' to extract from DataFrame\")\n",
    "        else:\n",
    "            self.columns = columns\n",
    "            self.dtypes = dtypes\n",
    "            self.np_return = np_return\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        if self.columns:\n",
    "            cols = self.columns\n",
    "        elif self.dtypes:\n",
    "            cols = X.select_dtypes(self.dtypes).columns.values\n",
    "        else:\n",
    "            raise ValueError(\"Can't transform with no columns or dtypes specified\")\n",
    "        \n",
    "        if self.np_return:\n",
    "            return X[cols].values\n",
    "        else:\n",
    "            return X[cols]\n",
    "\n",
    "class FixedLabelBinarizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    LabelBinarizer used to support being called with a dummy 'y', but in Scikit Learn 0.19 this was changed.\n",
    "    In 0.20 the class that will replace this is CategoricalEncoder.\n",
    "    Simply wrapped the LabelBinarizer to allow it to support the standard pipeline function stub.\n",
    "    Credit: Stackoverflow\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.encoder = LabelBinarizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.encoder.transform(X)\n",
    "\n",
    "class PassThroughTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transform function returns exactly the columns / features that are passed to it.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Discussion of Peek functionality:\n",
    "* Allows looking into pipeline data.\n",
    "* Using closure (factory pattern) to pass arguements to function within pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_factory(label, head=0, output_dtypes=True):\n",
    "    \"\"\"\n",
    "    Closure returning `peek` function that can be used to peek into the data in a pipeline\n",
    "    without affecting the pipeline functionality.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    label : str describing where in the pipeline we're peeking so that the log messages can be labelled\n",
    "    head : number of rows of data from the pipeline to output into the log\n",
    "    output_dtypes : for DataFrame only, whether or not to write dtypes into log\n",
    "    \n",
    "    Example usage:\n",
    "      FunctionTransformer( peek_factory(\"Look at XYZ\", head=1) )\n",
    "      \n",
    "    Note: supports looking at Panda's DataFrames or Numpy Arrays.\n",
    "    \"\"\"\n",
    "    def peek(X, label=label, head=head, output_dtypes=output_dtypes):\n",
    "        # Log information we want to know about the data passing through the pipeline:\n",
    "        logging.debug('{} - Type: {}'.format(label, type(X)))\n",
    "        logging.debug('{} - Shape: {}'.format(label, X.shape))\n",
    " \n",
    "        if type(X) == type(pd.DataFrame()):\n",
    "            if output_dtypes:\n",
    "                logging.debug('{} - Dtypes: {}'.format(label, X.dtypes))\n",
    "            if head > 0:\n",
    "                logging.debug('{} - Head:'.format(label))\n",
    "                tmp = X.head(head)\n",
    "                for (i,r) in enumerate(tmp.itertuples(index=False)):\n",
    "                    logging.debug('{} - Row {}: {}'.format(label, i, r))\n",
    "        else:\n",
    "            if head > 0:\n",
    "                logging.debug('{} - Head:'.format(label))\n",
    "                tmp = X[:head]\n",
    "                for (i,r) in enumerate(tmp):\n",
    "                    logging.debug('{} - Row {}: {}'.format(label, i, \",\".join(map(str, r))))\n",
    "        return X\n",
    "    return peek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of data: <a class=\"anchor\" id=\"prep-of-data\"></a>\n",
    "(Back to [agenda/toc](#agenda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.csv <==\r\n",
      "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\r",
      "\r\n",
      "1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\r",
      "\r\n",
      "2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C\r",
      "\r\n",
      "\r\n",
      "==> test.csv <==\r\n",
      "PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\r",
      "\r\n",
      "892,3,\"Kelly, Mr. James\",male,34.5,0,0,330911,7.8292,,Q\r",
      "\r\n",
      "893,3,\"Wilkes, Mrs. James (Ellen Needs)\",female,47,1,0,363272,7,,S\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Take a quick look at the top of the files to check what data we're getting as input:\n",
    "! head -3 train.csv test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in from input CSV files:\n",
    "df = pd.read_csv('train.csv', index_col='PassengerId')\n",
    "df_kaggle_holdout = pd.read_csv('test.csv', index_col='PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>PassengerId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>22</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket</th>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>7.25</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>7.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "PassengerId                        1  \\\n",
       "Survived                           0   \n",
       "Pclass                             3   \n",
       "Name         Braund, Mr. Owen Harris   \n",
       "Sex                             male   \n",
       "Age                               22   \n",
       "SibSp                              1   \n",
       "Parch                              0   \n",
       "Ticket                     A/5 21171   \n",
       "Fare                            7.25   \n",
       "Cabin                            NaN   \n",
       "Embarked                           S   \n",
       "\n",
       "PassengerId                                                  2  \\\n",
       "Survived                                                     1   \n",
       "Pclass                                                       1   \n",
       "Name         Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "Sex                                                     female   \n",
       "Age                                                         38   \n",
       "SibSp                                                        1   \n",
       "Parch                                                        0   \n",
       "Ticket                                                PC 17599   \n",
       "Fare                                                   71.2833   \n",
       "Cabin                                                      C85   \n",
       "Embarked                                                     C   \n",
       "\n",
       "PassengerId                       3  \n",
       "Survived                          1  \n",
       "Pclass                            3  \n",
       "Name         Heikkinen, Miss. Laina  \n",
       "Sex                          female  \n",
       "Age                              26  \n",
       "SibSp                             0  \n",
       "Parch                             0  \n",
       "Ticket             STON/O2. 3101282  \n",
       "Fare                          7.925  \n",
       "Cabin                           NaN  \n",
       "Embarked                          S  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split:\n",
    "X = df.drop('Survived',axis=1)\n",
    "y = df.Survived.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn Pipelines to simplify and modularize the data pipeline <a class=\"anchor\" id=\"sklearn-pipelines\"></a>\n",
    "(Back to [agenda/toc](#agenda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines provide a convenient construct for automating ML processes. They can be used to chain multiple estimators into a single estimator.\n",
    "<img src=\"images_2/pipeline.png\" style=\"height: 80px;\" align=\"center\"/>\n",
    "\n",
    "As described in Scikit Learn documentation:\n",
    "\n",
    "> Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\n",
    "\n",
    "Advantages of pipelines:\n",
    "* Modularize pipeline.\n",
    "* Simple to switch functions in / out of pipeline.\n",
    "* Very convenient for experimentation and searching for optimal ML approach.\n",
    "* Overcome common problems like data leakage in your test harness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Classes to know about:\n",
    "\n",
    "1. [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "> Pipeline(\\[ ('step1', Transformer1), ('step2', Transformer2), (...), ('model', Model) \\])\n",
    "<img src=\"images_2/pipeline.png\" style=\"height: 80px;\" align=\"center\"/>\n",
    "1. [FeatureUnion](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)\n",
    "> FeatureUnion(\\[ ('transformer1', Transformer1), ('transformer2', Transformer2), (...) \\])\n",
    "<img src=\"images_2/featureunion.png\" style=\"height: 210px;\" align=\"center\"/>\n",
    "1. [FunctionTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html)\n",
    "> FunctionTransformer(func, validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1: <a class=\"anchor\" id=\"exercise-1\"></a>\n",
    "(Back to [agenda/toc](#agenda))<br><br>\n",
    "We're going to spend some time looking at Pipelines built based on various Transformers and Estimators, let's start by getting a feeling for what these transformers do.<br><br>\n",
    "<span style=\"color:blue\">\n",
    "Use FunctionTransformer to:<br>\n",
    "    (1) Impute Fare values based on `fare_imputer` function, and<br>\n",
    "    (2) Impute Embarked values based on `embarked_imputer` function, and<br>\n",
    "    (3) Perform transformation using `custom_transformer` function.<br><br>\n",
    "</span>\n",
    "<div class=\"panel-group\" id=\"accordion-2\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse1-2\">Hints</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-2\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "For (1):<br>\n",
    "Use the function: `fare_imputer` as an argument to `FunctionTransformer()` use `validate=False` argument since we'll be passing a DataFrame into this transformer.<br><br>\n",
    "Call `fit_transform(X,y)` to perform transformation of the input feature matrix X.\n",
    "<br><br>\n",
    "Similar steps to solve (2) and (3), just with different functions: `embarked_imputer` and `custom_transformer`.\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse2-2\">Solution</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-2\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "One possible solution is:\n",
    "<br>\n",
    "`t = FunctionTransformer(fare_imputer, validate=False)\n",
    "print(\"Fare issues prior to transformation: {}\".format((df.Fare == 0).sum()))\n",
    "print(\"Fare issues after transformation: {}\".format((t.fit_transform(X,y).Fare == 0).sum()))`\n",
    "<br><br>\n",
    "`t = FunctionTransformer(embarked_imputer, validate=False)\n",
    "print(\"Embarked issues prior to transformation: {}\".format((df.Embarked.isnull()).sum()))\n",
    "print(\"Embarked issues after transformation: {}\".format((t.fit_transform(X,y).Embarked.isnull()).sum()))`\n",
    "<br><br>\n",
    "`t = FunctionTransformer(custom_transformer, validate=False)\n",
    "t.fit_transform(X,y).head().T`\n",
    "<br>\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fare issues prior to transformation: 15\n",
      "Fare issues after transformation: 0\n",
      "Embarked issues prior to transformation: 2\n",
      "Embarked issues after transformation: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>PassengerId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>7.25</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>7.925</td>\n",
       "      <td>53.1</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare_log</th>\n",
       "      <td>1.981</td>\n",
       "      <td>4.26666</td>\n",
       "      <td>2.07002</td>\n",
       "      <td>3.97218</td>\n",
       "      <td>2.08567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_16_to_34</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_35_to_47</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people_per_ticket</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people_per_lastname</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people_in_family</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cabin_level</th>\n",
       "      <td>O</td>\n",
       "      <td>C</td>\n",
       "      <td>O</td>\n",
       "      <td>C</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "PassengerId              1        2        3        4        5\n",
       "Pclass                   3        1        3        1        3\n",
       "Sex                   male   female   female   female     male\n",
       "Fare                  7.25  71.2833    7.925     53.1     8.05\n",
       "Embarked                 S        C        S        S        S\n",
       "fare_log             1.981  4.26666  2.07002  3.97218  2.08567\n",
       "age_16_to_34             1        0        1        0        0\n",
       "age_35_to_47             0        1        0        1        1\n",
       "age_missing              0        0        0        0        0\n",
       "people_per_ticket        1        1        1        2        1\n",
       "people_per_lastname      2        1        1        2        2\n",
       "people_in_family         2        2        1        2        1\n",
       "cabin_level              O        C        O        C        O"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise #1 solution:\n",
    "\n",
    "t = FunctionTransformer(fare_imputer, validate=False)\n",
    "print(\"Fare issues prior to transformation: {}\".format((df.Fare == 0).sum()))\n",
    "print(\"Fare issues after transformation: {}\".format((t.fit_transform(X,y).Fare == 0).sum())) \n",
    "\n",
    "t = FunctionTransformer(embarked_imputer, validate=False)\n",
    "print(\"Embarked issues prior to transformation: {}\".format((df.Embarked.isnull()).sum()))\n",
    "print(\"Embarked issues after transformation: {}\".format((t.fit_transform(X,y).Embarked.isnull()).sum())) \n",
    "\n",
    "t = FunctionTransformer(custom_transformer, validate=False)\n",
    "t.fit_transform(X,y).head().T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Pipeline: <a class=\"anchor\" id=\"simple-pipeline\"></a>\n",
    "(Back to [agenda/toc](#agenda))<br>\n",
    "* For illustration of how a pipeline works, let's create a \"simple\" pipeline which:\n",
    "  - Imputes missing values\n",
    "  - Peek at the data to see it's format\n",
    "  - Select just numeric columns -- otherwise Scikit Learn will error out\n",
    "  - Peek at the data again to see what's going into our model\n",
    "  - Apply a random forest model on the output\n",
    "  - Look at some basic model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rf_pipe = Pipeline(\n",
    "    [\n",
    "        ('impute_fare', FunctionTransformer(fare_imputer, validate=False)),\n",
    "        #('peek_before', FunctionTransformer(peek_factory('Prior to selection', head=1),validate=False)),\n",
    "        ('numerics_selection', FunctionTransformer(select_numeric_cols, validate=False)),\n",
    "        #('peek_after', FunctionTransformer(peek_factory('After selection', head=1),validate=False)),\n",
    "        ('pass_through', PassThroughTransformer()),\n",
    "        ('model', RandomForestClassifier(random_state=42))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline object created responds to standard Scikit Learn functions:\n",
    "* fit\n",
    "* fit_transform\n",
    "* predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model - Accuracy: 0.6815642458100558\n",
      "INFO:root:Model - F1 Score: 0.5648854961832062\n",
      "INFO:root:Model - Matthew's Corr Coef: 0.32718036166210035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_Died</th>\n",
       "      <th>Pred_Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_Died</th>\n",
       "      <td>85</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Survived</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Pred_Died  Pred_Survived\n",
       "Actual_Died             85             20\n",
       "Actual_Survived         37             37"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: dropping Age column as it includes NULLs causing Scikit-Learn to error out\n",
    "basic_rf_pipe.fit(X_train.drop('Age',axis=1),y_train)\n",
    "\n",
    "model_eval(y_test, basic_rf_pipe.predict(X_test.drop('Age',axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A couple of things to note:**\n",
    "* After pipeline setup, pipeline usage is just like using a normal Scikit Learn estimator: called `fit` and `predict`.\n",
    "* One can access steps in the pipeline using: `pipe.named_steps` this allows for detailed introspection of parameters / results from estimators within pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09885934, 0.08469302, 0.08979226, 0.72665538])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_rf_pipe.named_steps.model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complex Pipeline: <a class=\"anchor\" id=\"complex-pipeline\"></a>\n",
    "(Back to [agenda/toc](#agenda))<br><br>\n",
    "Planned pipeline:\n",
    "* Treating categorical attributes separately.\n",
    "* Using One-Hot encoding for catgorical attributes.\n",
    "<img src=\"images_2/complex_pipeline.png\" style=\"height: 450px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pclass = Pipeline(\n",
    "    [\n",
    "        ('selector_pclass', ColumnExtractor(columns = ['Pclass'])),\n",
    "        ('one_hot_pclass', FixedLabelBinarizer()),\n",
    "    ]\n",
    ")\n",
    "p_sex = Pipeline(\n",
    "    [\n",
    "        ('selector_sex', ColumnExtractor(columns = ['Sex'])),\n",
    "        ('one_hot_sex', FixedLabelBinarizer()),\n",
    "    ]\n",
    ")\n",
    "p_embarked = Pipeline(\n",
    "    [\n",
    "        ('selector_embarked', ColumnExtractor(columns = ['Embarked'])),\n",
    "        ('one_hot_embarked', FixedLabelBinarizer()),\n",
    "    ]\n",
    ")\n",
    "p_numeric = Pipeline(\n",
    "    [\n",
    "        ('selector_numeric', ColumnExtractor(dtypes = [np.int64, np.float64])),\n",
    "        ('scale_numeric', Normalizer()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "p_full = Pipeline(\n",
    "    [\n",
    "        ('embarked_imputer', FunctionTransformer(embarked_imputer, validate=False)),\n",
    "        ('fare_imputer', FunctionTransformer(fare_imputer, validate=False)),\n",
    "        ('custom_transform', FunctionTransformer(custom_transformer, validate=False)),\n",
    "        ('features', FeatureUnion(\n",
    "            [\n",
    "                ('pclass', p_pclass),\n",
    "                ('sex', p_sex),\n",
    "                ('embarked', p_embarked),\n",
    "                ('numeric', p_numeric),\n",
    "            ]\n",
    "        )),\n",
    "        ('estimators', FeatureUnion([\n",
    "            ('pass_through', PassThroughTransformer()),\n",
    "            ('m1', ModelTransformer(KNeighborsClassifier(n_neighbors=15))),\n",
    "            ('m2', ModelTransformer(GradientBoostingClassifier(random_state=42))),\n",
    "            ('m3', ModelTransformer(ElasticNet(random_state=42))),\n",
    "            ('m4', ModelTransformer(KMeans(n_clusters=3, random_state=42)))\n",
    "        ])),\n",
    "        #('final_peek', FunctionTransformer(peek_factory('Input to SVC', head=1))),\n",
    "        ('predictor', SVC())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training set evaluation - Accuracy: 0.9129213483146067\n",
      "INFO:root:Training set evaluation - F1 Score: 0.8755020080321284\n",
      "INFO:root:Training set evaluation - Matthew's Corr Coef: 0.8147383472327271\n",
      "INFO:root:Test set evaluation - Accuracy: 0.8156424581005587\n",
      "INFO:root:Test set evaluation - F1 Score: 0.7659574468085106\n",
      "INFO:root:Test set evaluation - Matthew's Corr Coef: 0.616565942969521\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_Died</th>\n",
       "      <th>Pred_Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_Died</th>\n",
       "      <td>92</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Survived</th>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Pred_Died  Pred_Survived\n",
       "Actual_Died             92             13\n",
       "Actual_Survived         20             54"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple fit, predict with pipeline:\n",
    "#   Large difference between ~10% between accuracy on Training set VS Test set ==> Overfitting\n",
    "p_model = p_full.fit(X_train,y_train)\n",
    "model_eval(y_train, p_model.predict(X_train), 'Training set evaluation')\n",
    "model_eval(y_test, p_model.predict(X_test), 'Test set evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7935300693047171"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation for quick CV assessment of model:\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "cross_val_score(p_full, X_train, y_train, cv=cv).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some take-aways from developing this section:\n",
    "Getting this \"more complex\" pipeline to work had some gotcha's.\n",
    "\n",
    "* The main challenge encountered was in trying to one-hot encode categorical variables as part of the Pipeline:\n",
    "  - Issue 1: `LabelBinarizer()` class (version 0.18) supported taking a Numpy array of text-based categorical variables and converting them to one-hot encoded columns but in version 0.19 the interface changed to not include optional `y` in the `transform()` function, thus deviating from the Pipeline interface.\n",
    "  - Issue 2: Using `pd.get_dummies` depends on the values present in the slice of data used during `fit` call, and also is order dependent.\n",
    "  \n",
    "* Solutions:\n",
    "  - For this tutorial, I patched `LabelBinarizer()` class to match the Pipeline interface using `FixedLabelBinarizer`.\n",
    "  - To use `pd.get_dummies`, one can combine setting a DataFrame column to a Categorical type using `pd.Categorical()` and then use pd.get_dummies as the Categorical definition ensures that the one-hot encoding of the column will always be consistent.\n",
    "  - In Scikit Learn version 0.20, a new `CategoricalEncoder` class will (hopefully) solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Grid Search / Randomized Search to optimize hyperparameters <a class=\"anchor\" id=\"search-for-hyperparameters\"></a>\n",
    "(Back to [agenda/toc](#agenda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes that allow us to search a parameter space to find the model parameters which give the best model performance.\n",
    "\n",
    "**High CPU usage ==> Higher model performance**\n",
    "\n",
    "1. [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "> GridSearchCV(estimator, param_grid, cv=None)\n",
    "\n",
    "1. [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "> RandomizedSearchCV(estimator, param_grid, cv=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at GridSearch on the [Simple Pipeline](#simple-pipeline) we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training set evaluation - Accuracy: 0.7528089887640449\n",
      "INFO:root:Training set evaluation - F1 Score: 0.6053811659192825\n",
      "INFO:root:Training set evaluation - Matthew's Corr Coef: 0.4552494131215883\n",
      "INFO:root:Test set evaluation - Accuracy: 0.7486033519553073\n",
      "INFO:root:Test set evaluation - F1 Score: 0.6280991735537189\n",
      "INFO:root:Test set evaluation - Matthew's Corr Coef: 0.47875642038386695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__max_depth': 5, 'model__min_samples_split': 9, 'model__n_estimators': 100}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_Died</th>\n",
       "      <th>Pred_Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_Died</th>\n",
       "      <td>96</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Survived</th>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Pred_Died  Pred_Survived\n",
       "Actual_Died             96              9\n",
       "Actual_Survived         36             38"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First example; hyper-parameter search on \"Simple Pipeline\":\n",
    "#   Specify parameters as: NAMED_STEP + \"__\" + PARAMETER_NAME\n",
    "simple_param_grid_1 = {\n",
    "    'model__n_estimators' : [100, 150, 200],\n",
    "    'model__max_depth' : [2, 3, 5],\n",
    "    'model__min_samples_split' : [3, 9, 15],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, random_state=42)\n",
    "\n",
    "gs_simple1 = GridSearchCV(basic_rf_pipe, simple_param_grid_1, cv=cv)\n",
    "gs_simple1.fit(X_train.drop('Age',axis=1), y_train)\n",
    "\n",
    "print(gs_simple1.best_params_)\n",
    "model_eval(y_train, gs_simple1.best_estimator_.predict(X_train.drop('Age',axis=1)), 'Training set evaluation')\n",
    "model_eval(y_test, gs_simple1.best_estimator_.predict(X_test.drop('Age',axis=1)), 'Test set evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2:  <a class=\"anchor\" id=\"exercise-2\"></a>\n",
    "(Back to [agenda/toc](#agenda))<br><br>\n",
    "It is possible to switch the entire model (or in fact any of the estimators) that's being used in the Pipeline. This can be done by providing the named step as a parameter directly; if we have different parameters related to the different models, we can have the parameter search across multiple SETs of parameters as follows:\n",
    "\n",
    "`param_grid = [\n",
    "    { grid 1 },\n",
    "    { grid 2 }, \n",
    "    ...\n",
    "]`<br><br>\n",
    "<span style=\"color:blue\">\n",
    "Let's modify the example above to:<br>\n",
    "* Search over a primary grid to switch to an `KNeighborsClassifier()` model and search over various values of `n_neighbors` parameter.\n",
    "* Also, define a secondary grid to switch to an `SVC()` model and search over various values of `C` parameter\n",
    "</span>\n",
    "<div class=\"panel-group\" id=\"accordion-3\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse1-3\">Hints</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "The model can be switched with the following parameter:<br>\n",
    "`'model' : [ MODEL_CLASS_INSTANTIATION ], `\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse2-3\">Solution</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "One possible solution is:\n",
    "<br>\n",
    "`simple_param_grid_ex2 = [\n",
    "    {\n",
    "         'model' : [ KNeighborsClassifier() ],\n",
    "         'model__n_neighbors' : [3, 7, 11, 15],\n",
    "    },\n",
    "    {\n",
    "        'model' : [ SVC(random_state=42) ],\n",
    "        'model__C' : [0.1, 0.3, 0.6, 0.9, 1.2],\n",
    "    }\n",
    "]`\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training set evaluation - Accuracy: 0.7752808988764045\n",
      "INFO:root:Training set evaluation - F1 Score: 0.641255605381166\n",
      "INFO:root:Training set evaluation - Matthew's Corr Coef: 0.5088081676064811\n",
      "INFO:root:Test set evaluation - Accuracy: 0.7374301675977654\n",
      "INFO:root:Test set evaluation - F1 Score: 0.6050420168067226\n",
      "INFO:root:Test set evaluation - Matthew's Corr Coef: 0.4549350970065187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': SVC(C=1.2, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'model__C': 1.2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_Died</th>\n",
       "      <th>Pred_Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_Died</th>\n",
       "      <td>96</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Survived</th>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Pred_Died  Pred_Survived\n",
       "Actual_Died             96              9\n",
       "Actual_Survived         38             36"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2:\n",
    "\n",
    "simple_param_grid_ex2 = [\n",
    "  {\n",
    "       'model' : [ KNeighborsClassifier() ],\n",
    "       'model__n_neighbors' : [3, 7, 11, 15],\n",
    "  },\n",
    "  {\n",
    "      'model' : [ SVC(random_state=42) ],\n",
    "      'model__C' : [0.1, 0.3, 0.6, 0.9, 1.2],\n",
    "  }\n",
    "]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, random_state=42)\n",
    "\n",
    "gs_simple_ex2 = GridSearchCV(basic_rf_pipe, simple_param_grid_ex2, cv=cv)\n",
    "gs_simple_ex2.fit(X_train.drop('Age',axis=1), y_train)\n",
    "\n",
    "print(gs_simple_ex2.best_params_)\n",
    "model_eval(y_train, gs_simple_ex2.best_estimator_.predict(X_train.drop('Age',axis=1)), 'Training set evaluation')\n",
    "model_eval(y_test, gs_simple_ex2.best_estimator_.predict(X_test.drop('Age',axis=1)), 'Test set evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What Parameters can be Searched Over?\n",
    "\n",
    "We can use: `estimator.get_params().keys()` to understand the parameters that can be set on a specific estimator / pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cv', 'error_score', 'estimator__memory', 'estimator__steps', 'estimator__impute_fare', 'estimator__numerics_selection', 'estimator__pass_through', 'estimator__model', 'estimator__impute_fare__accept_sparse', 'estimator__impute_fare__func', 'estimator__impute_fare__inv_kw_args', 'estimator__impute_fare__inverse_func', 'estimator__impute_fare__kw_args', 'estimator__impute_fare__pass_y', 'estimator__impute_fare__validate', 'estimator__numerics_selection__accept_sparse', 'estimator__numerics_selection__func', 'estimator__numerics_selection__inv_kw_args', 'estimator__numerics_selection__inverse_func', 'estimator__numerics_selection__kw_args', 'estimator__numerics_selection__pass_y', 'estimator__numerics_selection__validate', 'estimator__model__bootstrap', 'estimator__model__class_weight', 'estimator__model__criterion', 'estimator__model__max_depth', 'estimator__model__max_features', 'estimator__model__max_leaf_nodes', 'estimator__model__min_impurity_decrease', 'estimator__model__min_impurity_split', 'estimator__model__min_samples_leaf', 'estimator__model__min_samples_split', 'estimator__model__min_weight_fraction_leaf', 'estimator__model__n_estimators', 'estimator__model__n_jobs', 'estimator__model__oob_score', 'estimator__model__random_state', 'estimator__model__verbose', 'estimator__model__warm_start', 'estimator', 'fit_params', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example: \n",
    "gs_simple1.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing GridSearchCV / RandomizedSearchCV on [Complex Pipeline](#complex-pipeline) we developed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['estimators__m4',\n",
       " 'estimators__m4__model__algorithm',\n",
       " 'estimators__m4__model__copy_x',\n",
       " 'estimators__m4__model__init',\n",
       " 'estimators__m4__model__max_iter',\n",
       " 'estimators__m4__model__n_clusters',\n",
       " 'estimators__m4__model__n_init',\n",
       " 'estimators__m4__model__n_jobs',\n",
       " 'estimators__m4__model__precompute_distances',\n",
       " 'estimators__m4__model__random_state',\n",
       " 'estimators__m4__model__tol',\n",
       " 'estimators__m4__model__verbose',\n",
       " 'estimators__m4__model']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out what params can be tuned in pipeline:\n",
    "[i for i in p_full.get_params().keys() if 'm4' in i]\n",
    "#p_full.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=42, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('embarked_imputer', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function embarked_imputer at 0x7f17f7159bf8>,\n",
       "          inv_kw_args=None, inverse_func=None, kw_args=None,\n",
       "          pass_y='deprecated', validate=False)), ('fare_imputer', FunctionTransformer(accept_sparse=False,\n",
       " ...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=4,\n",
       "       param_grid=[{'predictor__C': [0.1, 0.3, 0.5, 1], 'estimators__m1__model__n_neighbors': [3, 7, 11, 15], 'estimators__m2__model__n_estimators': [100, 150, 200]}, {'predictor': [RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=3, max_features='auto', max...7, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0))]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter search:\n",
    "cv = StratifiedKFold(n_splits=3, random_state=42)\n",
    "p1_grid =[\n",
    "    {\n",
    "        'predictor__C' : [0.1, 0.3, 0.5, 1],\n",
    "        'estimators__m1__model__n_neighbors' : [3,7,11,15],\n",
    "        'estimators__m2__model__n_estimators' : [100, 150, 200],\n",
    "    },\n",
    "    {\n",
    "        'predictor' : [ RandomForestClassifier(random_state=42) ],\n",
    "        'predictor__n_estimators' : [10, 25],\n",
    "        'predictor__max_depth' : [3, 5, 7],\n",
    "        'estimators__m1' : [None, ModelTransformer(KNeighborsClassifier(n_neighbors=15))],\n",
    "        'estimators__m2' : [None, ModelTransformer(RandomForestClassifier(random_state=42))],\n",
    "        'estimators__m3' : [None, ModelTransformer(GradientBoostingClassifier(random_state=42))],\n",
    "        'estimators__m4' : [None, ModelTransformer(KMeans(n_clusters=7, random_state=42))],\n",
    "    },\n",
    "]\n",
    "\n",
    "gs1 = GridSearchCV(p_full, p1_grid, cv=cv, n_jobs=4)\n",
    "gs1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model - Accuracy: 0.9129213483146067\n",
      "INFO:root:Model - F1 Score: 0.8755020080321284\n",
      "INFO:root:Model - Matthew's Corr Coef: 0.8147383472327271\n",
      "INFO:root:Model - Accuracy: 0.8156424581005587\n",
      "INFO:root:Model - F1 Score: 0.7659574468085106\n",
      "INFO:root:Model - Matthew's Corr Coef: 0.616565942969521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimators__m1': None, 'estimators__m2': None, 'estimators__m3': ModelTransformer(model=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)), 'estimators__m4': ModelTransformer(model=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=7, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)), 'predictor': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False), 'predictor__max_depth': 3, 'predictor__n_estimators': 10}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_Died</th>\n",
       "      <th>Pred_Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_Died</th>\n",
       "      <td>92</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Survived</th>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Pred_Died  Pred_Survived\n",
       "Actual_Died             92             13\n",
       "Actual_Survived         20             54"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gs1.best_params_)\n",
    "model_eval(y_train,gs1.best_estimator_.predict(X_train))\n",
    "model_eval(y_test,gs1.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding which models are overfitting / underfitting <a class=\"anchor\" id=\"overfitting-underfitting\"></a>\n",
    "(Back to [agenda/toc](#agenda))<br><br>\n",
    "\n",
    "<img src=\"images_2/overfit_underfit.png\" style=\"height: 180px;\" align=\"center\"/>\n",
    "\n",
    "Standard approach for understanding whether model is overfitting or underfitting is by looking at the prediction error on the training data and on the evaluation (test) data.\n",
    "\n",
    "**Underfitting:** Poor performance on training data ==> model is too simple, increase model flexibility:\n",
    "* Add new domain-specific features and more feature Cartesian products, and change the types of feature processing used\n",
    "* Decrease the amount of regularization used\n",
    "\n",
    "**Overfitting:** Good performance of training data, but not generalizable to evaluation (test) data ==> reduce model flexibility\n",
    "* Feature selection: consider using fewer feature combinations, and decrease the number of numeric attribute bins\n",
    "* Increase the amount of regularization used\n",
    "\n",
    "Accuracy on training and test data could be poor because the learning algorithm did not have enough data to learn from. You could improve performance by doing the following:\n",
    "* Increase the amount of training data examples\n",
    "* Increase the number of passes on the existing training data\n",
    "\n",
    "*Source: Amazon AWS Documentation* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Kaggle Submission file based on predicted outputs: <a class=\"anchor\" id=\"kaggle-submission\"></a>\n",
    "(Back to [agenda/toc](#agenda))<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're playing around with this notebook and want to submit a solution on Kaggle,\n",
    "#  you can use the below to create the submission file.\n",
    "pd.DataFrame(\n",
    "    data=gs1.best_estimator_.predict(df_kaggle_holdout),\n",
    "    index=df_kaggle_holdout.index,\n",
    "    columns=['Survived']).reset_index().to_csv('nova_submission_session2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3 - Homework: <a class=\"anchor\" id=\"exercise-3\"></a>\n",
    "(Back to [agenda/toc](#agenda))<br><br>\n",
    "<span style=\"color:blue\">\n",
    "Use this notebook as a starting point, and see if you can get the accuracy of the model higher through:\n",
    "* Adding new features\n",
    "* Modifying the pipeline\n",
    "\n",
    "We'd love to hear from you on [Slack](http://www.slack.com) (\"NovaDataScience\" group) with discussion of any progress you've made, issues you've encountered, or stories of success or failure digging into this problem further.\n",
    "</span>\n",
    "<div class=\"panel-group\" id=\"accordion-4\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse1-4\">Hints</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-4\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "Some ideas for features to add:<br>\n",
    "<ul>\n",
    "  <li>Extract of title from Name attribute.</li>\n",
    "  <li>Combined features related to position in family.</li>\n",
    "  <li>Other thoughts?</li>\n",
    "</ul>\n",
    "Some ideas for possible pipeline modifications:<br>\n",
    "<ul>\n",
    "  <li>Addition of `PCA()` components in a separate pipeline.</li>\n",
    "  <li>Selection of features to use in model using `SelectKBest()`.</li>\n",
    "  <li>Other thoughts?</li>\n",
    "</ul>\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = FunctionTransformer(transformer_dummies, kw_args = {'columns' : ['Sex', 'Embarked', 'Pclass']}, validate=False)\n",
    "t.transform(df[['Pclass','Sex','Embarked']]).head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pipeline([\n",
    "    ('impute_fare', FunctionTransformer(fare_imputer,validate=False)),\n",
    "    ('impute_embarked', FunctionTransformer(embarked_imputer,validate=False)),\n",
    "    ('custom', FunctionTransformer(custom_transformer,validate=False)),\n",
    "    ('fUnion', FeatureUnion([\n",
    "        ('categoricalPipeline', Pipeline([\n",
    "            ('categoricalSelector', FunctionTransformer(columns_subset,\n",
    "                                                        kw_args={ 'columns' : ['Sex','Pclass','Embarked', 'cabin_level']},\n",
    "                                                        validate=False)),\n",
    "            ('encoder', FunctionTransformer(transformer_dummies, kw_args = {'columns' : ['Sex', 'Pclass', 'Embarked', 'cabin_level']}, validate=False))\n",
    "        ])),\n",
    "        ('farePipeline', Pipeline([\n",
    "            ('fareSelector', FunctionTransformer(columns_subset,\n",
    "                                                        kw_args={ 'columns' : ['Fare']},\n",
    "                                                        validate=False)),\n",
    "            ('fareNorm', MinMaxScaler())\n",
    "        ])),\n",
    "        ('colSelector', FunctionTransformer(columns_subset,\n",
    "                                                        kw_args={ 'columns' : ['fare_log', 'age_16_to_34',\n",
    "                                                                              'age_35_to_47', 'age_missing',\n",
    "                                                                              'people_per_ticket', 'people_per_lastname',\n",
    "                                                                              'people_in_family']},\n",
    "                                                        validate=False)),\n",
    "    ])),\n",
    "    ('model', SVC())\n",
    "])\n",
    "\n",
    "X = df.drop(['Survived'],axis=1)\n",
    "y = df.Survived.values\n",
    "m = p.fit(X, y)\n",
    "m.predict(df_kaggle_holdout)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

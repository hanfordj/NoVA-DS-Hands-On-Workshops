{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 101 Hands On Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data:\n",
    "#  This will pull the required data (train.csv and test.csv files) from dropbox.\n",
    "#  The cool thing is that this is leveraging wget on the docker container to pull data to your local disk,\n",
    "#  consistently for all operating systems.\n",
    "! rm -f train.csv test.csv\n",
    "! wget https://www.dropbox.com/s/f7fb3gon8byyyz6/train.csv?dl=1 -O train.csv -q\n",
    "! wget https://www.dropbox.com/s/zcd2751x6waex9f/test.csv?dl=1 -O test.csv -q\n",
    "! ls -l train.csv test.csv\n",
    "! wc -l train.csv test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"panel-group\" id=\"accordion-1\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-1\">Introduction / Motivation</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-1\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "      <h3 class=\"panel-title\">Why are you here?</h3>\n",
    "      <img src='images/sexiest_job.png' style=\"width: 500px;\" align=\"right\"/>\n",
    "      <ul>\n",
    "          <li>Want to check out this whole \"**Data Science - Sexiest Job of the 21st Century**\" thing.</li>\n",
    "          <li>Free pizza?</li>\n",
    "          <li>Have been dabbling/reading, but are struggling to go through a **data science problem start to finish**.</li>\n",
    "          <li>Have been using XYZ (R, SAS, Matlab, Octave, etc.) toolset, but would like to **check out Python as an alternative**.</li>\n",
    "          <li>Others?</li>\n",
    "      </ul>\n",
    "      <h3>Motivation for doing this workshop</h3>\n",
    "      <ul>\n",
    "          <li>Attended many meetups talking through interesting but potentially intractible problems for a beginner.</li>\n",
    "          <li>Saw many developers / analysts who wanted to move deeper into DS, but struggling to find good local content. </li>\n",
    "          <li>Wanted to see more hands-on approach (walk out and feel you can start writing code or at least have a starting point).</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "<div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse2-1\">Environment Details</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-1\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "      <img src='images/docker_icon.png' style=\"width: 100px;\" align=\"right\"/>\n",
    "      <ul>\n",
    "          <li>Docker</li>\n",
    "          <ul type=\"circle\">\n",
    "              <li>Isolation, consistency</li>\n",
    "              <li>Includes installation of Python 3.6.3</li>\n",
    "              <li>Additional modules: jupyter, numpy, pandas, matplotlib, sklearn, seaborn</li>\n",
    "          </ul>\n",
    "       </ul>\n",
    "       <img src='images/jupyter_icon.png' style=\"width: 100px;\" align=\"right\"/>\n",
    "       <ul>\n",
    "           <li>Jupyter notebook:</li>\n",
    "           <ul type=\"circle\">\n",
    "             <li>Quick feedback loop</li>\n",
    "             <li>Writeup and code co-located (like R-Markdown files, but simpler)</li>\n",
    "             <li>Easy interface</li>\n",
    "           </ul>\n",
    "       </ul>\n",
    "       <ul>\n",
    "           <li>Quick overview of getting this environment running:</li>\n",
    "           <ul>\n",
    "               <li>Install docker</li>\n",
    "               <li>Download docker image:\n",
    "  \n",
    "    `docker login -u <USERNAME>`\n",
    "\n",
    "    `docker pull hanfordj/ds_jupyter_launcher`\n",
    "</li>\n",
    "               <li>Run docker image:\n",
    "  \n",
    "    `docker run -d --name ds_jupyter_1 -P -v ~/docker/mnt:/ds hanfordj/ds_jupyter_launcher`\n",
    "</li>\n",
    "               <li>Access Jupyter Notebook at: `http://localhost:XXXX` (where XXXX = mapped port id)</li>\n",
    "            </ul>\n",
    "       </ul>\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse3-1\">Real World ML Pipeline</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-1\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "Involves a number of complicated steps, we'll just be focusing on the preprocessing and modeling steps highlighted below:\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Understanding of business problem</li>\n",
    "        <li>Problem formalization</li>\n",
    "        <li>Data collection</li>\n",
    "        <li>**Data preprocessing / cleansing**</li>\n",
    "        <li>**Modelling**</li>\n",
    "        <li>Way to evaluate model in real life</li>\n",
    "        <li>Model deployment</li>\n",
    "        <li>Model monitoring and retuning</li>\n",
    "    </ul>\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process we'll be following:\n",
    "1. Read data\n",
    "1. Exploratory analysis of data\n",
    "1. Feature engineering\n",
    "1. Model Creation\n",
    "1. Cross validation\n",
    "1. Grid Search to find Optimal Model Parameters\n",
    "1. Model Evaluation\n",
    "1. Kaggle Submission\n",
    "1. Interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing _1502_ out of _2224_ passengers and crew (67.54 % died). This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "<img src='images/sunken-titanic.jpeg' style=\"width: 500px;\"/>\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this problem, we:\n",
    "* Analyze the sorts of people that were likely to survive.\n",
    "* Apply the tools of machine learning to predict which passengers survived the tragedy (classification problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Min overview of the Problem:\n",
    "* Go here for details: https://www.kaggle.com/c/titanic\n",
    "* Evaluation metric: Accuracy -- % of passengers you correctly predict.\n",
    "* Submission file format: ! head -10 gender_submission.csv\n",
    "* Data:\n",
    "  - train.csv = Training data (incl target variable)\n",
    "  - test.csv = Testing data (excl target variable)\n",
    "* Data dictionary:\n",
    "  <img src='images/data_dictionary.png' style=\"width: 800px;\"/>\n",
    "* Variable notes:\n",
    "  <img src='images/variable_notes.png' style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main Python libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, Imputer, MinMaxScaler, Normalizer, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a quick look at the top of the files to check what data we're getting as input:\n",
    "! head -3 train.csv test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1:\n",
    "<span style=\"color:blue\">\n",
    "Here we'd like to read data from the CSV files into Panda's DataFrame objects.\n",
    "* Please read \"train.csv\" into a DataFrame named as \"df\".\n",
    "* Please read \"test.csv\" into a DataFrame named as \"df_test\".\n",
    "</span>\n",
    "<div class=\"panel-group\" id=\"accordion-2\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse1-2\">Hints</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-2\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "Use the function: `pd.read_csv(...)`\n",
    "<br><br>\n",
    "To get additional information about a function within Jupyter, open a new cell (ESC + B), type: `<fxn_name>?` and run the cell, so for example in this case:\n",
    "<br>\n",
    "Type: `pd.read_csv?`\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse2-2\">Solution</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-2\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "One possible solution is:\n",
    "<br>\n",
    "`df = pd.read_csv('train.csv')`\n",
    "<br>\n",
    "`df_test = pd.read_csv('test.csv')`\n",
    "<br>\n",
    "The solution is so simple in this case as the fields have comma delimiters and double quote qualifiers (the default), but in some cases additional parameters will be needed to read data into a DataFrame.\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in from input CSV files (Exercise #1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at top 5 rows from each DataFrame (using head function):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2:\n",
    "<span style=\"color:blue\">\n",
    "Here we'd like to inspect the data that's been read in, check:\n",
    "* Column data types\n",
    "* Number of rows within each data frame\n",
    "* Whether there are NULLs present (missing values) in the data\n",
    "</span>\n",
    "<div class=\"panel-group\" id=\"accordion-3\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse1-3\">Hints</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "Try the functions: `__dataFrame__.info()` and `__dataFrame__.describe()`\n",
    "<br>\n",
    "you could also try using some of the other functions:\n",
    "<ul>\n",
    "  <li>Python's `len()` function to calculate the length of the data frame.</li>\n",
    "  <li>Panda's `__dataFrame__.shape` attribute to see the dimensionality of the data frame.</li>\n",
    "  <li>Panda's `__dataFrame__.isnull()` function to determine the null's within a data frame.</li>\n",
    "</ul>\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse2-3\">Solution</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "One possible solution is:\n",
    "<br>\n",
    "`df_to_inspect = df` # Can switch to \"df_test\" too...\n",
    "<br><br>\n",
    "`print(df_to_inspect.info())`<br>\n",
    "`print()`<br>\n",
    "`print(df_to_inspect.describe())`<br>\n",
    "`print()`<br>\n",
    "`print(\"Length of DataFrame is: {}\".format(len(df_to_inspect)))`<br>\n",
    "`print(\"Shape of DataFrame is: {}\".format(df_to_inspect.shape))`<br>\n",
    "`print(\"\\nNULL counts per column within DataFrame are given below: \\n\\n{}\".format(df_to_inspect.isnull().sum()))`\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the data we just read in (Exercise #2):\n",
    "\n",
    "# Look at column data types:\n",
    "\n",
    "\n",
    "# Number of rows within the DataFrames (bonus - number of columns):\n",
    "\n",
    "\n",
    "# Count NULLs (missing values) in the DataFrames: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined data frame for exploration and data cleansing:\n",
    "combined_df = pd.concat([df,df_test], ignore_index=True)\n",
    "print( \"Shape: {}\".format( combined_df.shape ) )\n",
    "print( \"Training: {}\".format( combined_df[combined_df.Survived.notnull()].shape ) )\n",
    "print( \"Submission Test: {}\".format( combined_df[combined_df.Survived.isnull()].shape ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of null values for Train / Test:\n",
    "combined_df.isnull().applymap(int).rename({'Survived':'Test Set'},axis=1).groupby('Test Set').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some discussion points:\n",
    "* _Does it make sense to analyze the data as a combined dataframe (train + test)?_\n",
    "* _Why do we care about missing values?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of important variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survival (Target variable):\n",
    "Survival rate in the training dataset is at approximately 38.4% (for the 891 people on the Training Set). The Survival of the Test set is what we're trying to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3:\n",
    "<span style=\"color:blue\">\n",
    "Here we'd like to use the training data \"df\" DataFrame to calculate the \"survival rate\"; this can be calculated as:\n",
    "* 100.0 * {number of rows where Survived == 1} / {total number of rows}\n",
    "* Round the result off to one decimal place\n",
    "</span>\n",
    "<div class=\"panel-group\" id=\"accordion-4\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse1-4\">Hints</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-4\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "You can access the values of a column from the dataframe using two approaches:\n",
    "<ul>\n",
    "    <li>__dataFrame__.__columnName__       e.g. df.Survived</li>\n",
    "    <li>__dataFrame__['__columnName__']    e.g. df['Survived']</li>\n",
    "</ul>\n",
    "Checking such a column using relational operators: ==, !=, &lt;, &gt;, etc. will return a boolean Pandas Series, stating for each row the evaluation of the operation.\n",
    "<br><br>\n",
    "E.g. df.Survived == 1 will return [True, False, ..., True].\n",
    "<br><br>\n",
    "One can apply aggregation functions to such a boolean Series to get summary statistics back regarding the array:\n",
    "<br>\n",
    "<ul>\n",
    "    <li>__series__.sum()    --> returns sum of elements, True = 1, False = 0</li>\n",
    "    <li>__series__.count()  --> returns count of elements</li>\n",
    "    <li>__series__.max()    --> returns max value from series</li>\n",
    "    <li>__series__.min()    --> returns min value from series</li>\n",
    "    <li>__series__.mean()   --> returns mean calculated from series</li>\n",
    "</ul>\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse2-4\">Solution</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-4\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "Some possible solutions:\n",
    "<br>\n",
    "<ul>\n",
    "    <li>number_of_rows_where_survived_eq_1 = (df.Survived == 1).sum()</li>\n",
    "    <li>number_of_rows_where_survived_eq_1 = (df.Survived > 0).sum()</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>total_num_rows = len(df)</li>\n",
    "    <li>total_num_rows = df.shape[0]</li>\n",
    "    <li>total_num_rows = df.Survived.count()</li>\n",
    "</ul>\n",
    "And maybe a simpler way:\n",
    "`rate = round(100.0*(df.Survived == 1).mean(),1)`\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_where_survived_eq_1 =\n",
    "print(number_of_rows_where_survived_eq_1)\n",
    "\n",
    "total_num_rows = \n",
    "print(total_num_rows)\n",
    "\n",
    "rate = round(100.0*number_of_rows_where_survived_eq_1/total_num_rows,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "data = df.groupby(['Survived']).size()\n",
    "data.plot(kind='bar',title='Survival Rate (Train Only) = {}%'.format(rate),ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender (Sex):\n",
    "* Gender distribution consistent between train and test sets (no sampling bias).\n",
    "* Female survival rate significantly higher than for males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,10))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "rate_female = round(100.0*(combined_df.Sex=='female').sum()/combined_df.Sex.count(),1)\n",
    "data = combined_df.groupby(['Sex']).size()\n",
    "data.plot(kind='bar',title='Overall Gender Distribution - {}% Female'.format(rate_female),ax=ax1);\n",
    "\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "rate_female = round(100.0*(df.Sex=='female').sum()/df.Sex.count(),1)\n",
    "data = combined_df.groupby(['Sex']).size()\n",
    "data.plot(kind='bar',title='Gender Distribution (Train Only) - {}% Female'.format(rate_female),ax=ax3);\n",
    "\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "data = df.groupby(['Sex', 'Survived']).size()\n",
    "data.unstack().plot(kind='bar',title='Impact of Gender on Survival (Train Only)',ax=ax4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passenger Class (Pclass):\n",
    "* Passenger Class distribution consistent between train and test sets (no sampling bias).\n",
    "* Survival rates by Pclass:\n",
    "  - 1: good survival rate (approx. 2-to-1 odds of surviving)\n",
    "  - 2: flip of a coin\n",
    "  - 3: really poor survival rate (approx. 1-in-4 survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,10))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "(rate_P1,rate_P2,rate_P3) = (round(100.0*(combined_df.Pclass==1).sum()/combined_df.Pclass.count(),1),\n",
    "                             round(100.0*(combined_df.Pclass==2).sum()/combined_df.Pclass.count(),1),\n",
    "                             round(100.0*(combined_df.Pclass==3).sum()/combined_df.Pclass.count(),1))\n",
    "data = combined_df.groupby(['Pclass']).size()\n",
    "data.plot(kind='bar',title='Overall Class Distribution - (P1={}%, P2={}%, P3={}%)'.format(rate_P1,rate_P2,rate_P3),ax=ax1);\n",
    "\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "(rate_P1,rate_P2,rate_P3) = (round(100.0*(df.Pclass==1).sum()/df.Pclass.count(),1),\n",
    "                             round(100.0*(df.Pclass==2).sum()/df.Pclass.count(),1),\n",
    "                             round(100.0*(df.Pclass==3).sum()/df.Pclass.count(),1))\n",
    "data = combined_df.groupby(['Pclass']).size()\n",
    "data.plot(kind='bar',title='Class Distribution (Train Only) - (P1={}%, P2={}%, P3={}%)'.format(rate_P1,rate_P2,rate_P3),ax=ax3);\n",
    "\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "data = df.groupby(['Pclass', 'Survived']).size()\n",
    "data.unstack().plot(kind='bar',title='Impact of Passenger Class on Survival (Train Only)',ax=ax4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactions between Gender and Passenger Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "df1 = df.copy()\n",
    "df1['GenderClass'] = df1.apply(lambda x: '{}{}'.format(x.Sex[0:1].upper(),x.Pclass),axis=1)\n",
    "data = df1.groupby([ 'GenderClass', 'Survived']).size()\n",
    "data.unstack().plot(kind='bar',title='Impact of Gender and Passenger Class on Survival (Train Only)',ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Exploration and Missing Value Handling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age:\n",
    "* High number of missing values within Age variable.\n",
    "* There is certainly some information in this variable:\n",
    "  - High death rate in middle-aged band.\n",
    "  - Youth and older passengers more likely to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket Age to convert it into a Categorical Variable:\n",
    "combined_df['AgeBuckets'] = pd.cut(combined_df.Age,bins=5)\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax1 = fig.add_subplot(3,1,1)\n",
    "data = combined_df.Age[~combined_df.Age.isnull()].apply(int).value_counts().sort_index()\n",
    "data.plot(kind='bar',title='Overall Age Distribution',ax=ax1)\n",
    "ax1.set_xticks([i for i in range(0,85,5)]);\n",
    "ax1.set_xticklabels([i for i in range(0,85,5)]);\n",
    "\n",
    "ax3 = fig.add_subplot(3,1,2)\n",
    "data = combined_df[~combined_df.Age.isnull()].groupby(['Survived']).Age\n",
    "data.plot(kind='density',title='Age Density Plots by Survival',ax=ax3);\n",
    "ax3.legend(title='Survived');\n",
    "\n",
    "ax2 = fig.add_subplot(3,1,3)\n",
    "data = combined_df.groupby(['AgeBuckets','Survived']).size()\n",
    "data.unstack().plot(kind='bar',title='Impact of Bucketed Age on Survival',ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rather than trying to impute a large number of missing values,\n",
    "#  we'll just use the informative middle-aged band as a variable:\n",
    "\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "data = combined_df[['Age','Survived']].dropna().astype(int).groupby(['Age','Survived']).size()\n",
    "data.unstack().plot(kind='bar',ax=ax1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['age_16_to_34'] = ((combined_df.Age >= 16) & (combined_df.Age <= 34)).apply(int)\n",
    "combined_df['age_35_to_47'] = ((combined_df.Age >= 16) & (combined_df.Age <= 47)).apply(int)\n",
    "print(combined_df['age_16_to_34'].head())\n",
    "print(combined_df['age_35_to_47'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns\n",
    "combined_df.drop(['Age','AgeBuckets'],axis=1,inplace=True)\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fare:\n",
    "* Right skewed distribution.\n",
    "* Can either normalize, or bin to make this variable more usable in an ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute Fare values for Null and 0 fare's:\n",
    "combined_df[(combined_df.Fare == 0) | (combined_df.Fare.isnull()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Median Pclass Fare to Impute missing values:\n",
    "pclass_to_fare_lookup = combined_df[~((combined_df.Fare == 0) | (combined_df.Fare.isnull())) ].groupby(['Pclass']).Fare.median()\n",
    "combined_df.loc[((combined_df.Fare == 0) | (combined_df.Fare.isnull())), 'Fare'] = \\\n",
    "    combined_df['Pclass'].map(dict(pclass_to_fare_lookup))\n",
    "\n",
    "print('Imputed using the mapping: {}'.format(dict(pclass_to_fare_lookup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that the imputation worked:\n",
    "combined_df[(combined_df.Fare == 0) | (combined_df.Fare.isnull()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,10))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "combined_df.Fare.plot(kind='hist',title='Overall Fare Distribution',ax=ax1);\n",
    "\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "data = np.log(combined_df.Fare)\n",
    "data.plot(kind='hist',title='Log Distribution - ML Input',ax=ax3);\n",
    "\n",
    "#df['FareBuckets'] = pd.cut(combined_df.Fare,bins=5)\n",
    "df['FareBuckets'] = pd.qcut(combined_df.Fare,q=[0, .2, .4, .6, .8, 1.])\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "data = df.groupby(['FareBuckets', 'Survived']).size()\n",
    "data.unstack().plot(kind='bar',title='Impact of Fare on Survival (Train Only)',ax=ax4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Variables Quick Treatments:\n",
    "* Drop \"Cabin\" variable\n",
    "* Impute missing values for \"Embarked\"\n",
    "* Add some additional features related to counting people per Ticket, per Family and per Last Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Cabin -- decided to just leave this one out this go around:\n",
    "combined_df.drop(['Cabin'],axis=1,inplace=True)\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute Embarked:\n",
    "combined_df[combined_df.Embarked.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare by Embarkation point:\n",
    "pd.DataFrame(combined_df.groupby(['Embarked']).Fare.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass by Embarkation Point:\n",
    "pd.DataFrame(combined_df.groupby(['Embarked','Pclass']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare statistics for the various Embarkation points:\n",
    "pd.DataFrame(combined_df[combined_df.Pclass==1].groupby(['Embarked']).Fare.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.Embarked.fillna('S',inplace=True)\n",
    "combined_df[combined_df.Embarked.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable; people_per_ticket (based on \"Ticket\" input variable):\n",
    "combined_df = combined_df.merge(pd.DataFrame(combined_df.groupby('Ticket').size(),\n",
    "                                             columns=['people_per_ticket']),\n",
    "                                how='left', left_on='Ticket', right_index=True)\n",
    "combined_df.drop(['Ticket'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable; people_per_lastname (based on \"Name\" input variable):\n",
    "combined_df['Lastname'] = combined_df.Name.apply(lambda x: x.split(',')[0])\n",
    "combined_df = combined_df.merge(pd.DataFrame(combined_df.groupby('Lastname').size(),\n",
    "                                             columns=['people_per_lastname']),\n",
    "                                how='left', left_on='Lastname', right_index=True)\n",
    "combined_df.drop(['Name','Lastname'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable; people in family based on \"Parch\" and \"SibSp\" input variables:\n",
    "combined_df['people_in_family'] = combined_df.apply(lambda x: x.Parch+x.SibSp+1,axis=1)\n",
    "combined_df.drop(['Parch','SibSp'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double check that all data is imputed (no missing values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding of values for ML input:\n",
    "* Most models do not accept Text or any other non-numerical data as input.\n",
    "* The below makes the necessary changes to encode the input features in a format that can be handled by Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panda's get_dummies function is really convenient for encoding categorical variables using one-hot encoding:\n",
    "combined_df = \\\n",
    "pd.get_dummies(combined_df,\n",
    "               prefix = {'Sex': 'sex', 'Embarked': 'embarked', 'Pclass': 'pclass'},\n",
    "               drop_first=True,\n",
    "               columns = ['Sex', 'Embarked', 'Pclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn also provides similar functionality, but the marshalling of such values back into the DataFrame adds some complexity:\n",
    "#encoder = LabelBinarizer()\n",
    "#combined_df['sex']=encoder.fit_transform(combined_df.Sex)\n",
    "#combined_df.groupby(['Sex','sex']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log of Fare value to normalize range:\n",
    "combined_df['fareLog'] = np.log(combined_df.Fare)\n",
    "# Standard Scaled version of Fare:\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(combined_df[combined_df.Survived.notnull()].Fare.values.reshape(-1,1))\n",
    "combined_df['fareScaled'] = scaler.transform(combined_df.Fare.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[combined_df.Survived.notnull()].fareScaled.describe() # --> Mean = 0, Std Deviation = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at distributions of Fare features:\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "combined_df.fareLog.plot(kind='hist', title='Log Normalization', ax=ax1);\n",
    "combined_df.fareScaled.plot(kind='hist',title='StandardScaler Normalization', ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop(['Fare', 'PassengerId'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Model Building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep Training, Test, Validation Datasets:\n",
    "X = combined_df[combined_df.Survived.notnull()].drop(['Survived'],axis=1)\n",
    "y = combined_df[combined_df.Survived.notnull()]['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, train_size = 0.6, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X, c=y, figsize=(15,15), alpha=0.8, marker='o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "c = X_train.corr()\n",
    "mat_ax = ax.matshow(c);\n",
    "fig.colorbar(mat_ax)\n",
    "plt.xticks(range(len(c.columns)), c.columns, rotation=45);\n",
    "plt.yticks(range(len(c.columns)), c.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, max_depth=4, random_state=42)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix:\n",
    "y_actual = y_test\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_actual, y_pred)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_actual, y_pred)))\n",
    "print(\"Matthew's Corr Coef: {}\".format(matthews_corrcoef(y_actual, y_pred)))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_actual, y_pred),\n",
    "             columns=['Pred_Died','Pred_Survived'],\n",
    "             index=['Actual_Died','Actual_Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #4:\n",
    "<span style=\"color:blue\">\n",
    "Let's use Scikit Learn to build another model (named as \"svm\") - this time a Support Vector Machine. Remember that the workflow for Scikit Learn is: `Instatiate model -- Fit to data -- Predict`\n",
    "<ul>\n",
    "    <li>Instantiate model class: SVC()</li>\n",
    "    <li>Fit to training data</li>\n",
    "    <li>Evaluate using confusion matrix / performance metrics</li>\n",
    "</ul>\n",
    "</span>\n",
    "<div class=\"panel-group\" id=\"accordion-5\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse1-5\">Hints</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-5\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "Fitting to the training data can be done using: `svm.fit()` function - the data to fit with is: X_train, y_train.\n",
    "<br><br>\n",
    "To evaluate the model, we compare the y_test data (actual ground truth) against the predicted data; obtained using `svm.predict()` function.\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse2-5\">Solution</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-5\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "One possible solution:\n",
    "<br><br>\n",
    "# Model creation and fitting:<br>\n",
    "`svm = SVC()`<br>\n",
    "`svm.fit(X_train,y_train)`<br>\n",
    "<br>\n",
    "# Model evaluation:<br>\n",
    "`y_actual = y_test`<br>\n",
    "`y_pred = svm.predict(X_test)`\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation and fitting:\n",
    "svm = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check prediction accuracy using test set:\n",
    "y_actual = \n",
    "y_pred = \n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_actual, y_pred)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_actual, y_pred)))\n",
    "print(\"Matthew's Corr Coef: {}\".format(matthews_corrcoef(y_actual, y_pred)))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_actual, y_pred),\n",
    "             columns=['Pred_Died','Pred_Survived'],\n",
    "             index=['Actual_Died','Actual_Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Questions:_\n",
    "* _How did the SVM model compare with the Random Forest model?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Cross Validation:\n",
    "* Applies fit on non-test folds of data and evaluates accuracy on the last fold of the data.\n",
    "* With cv=5 it's fitting the model 5 times to 4/5 of the data and then evaluating on the last 1/5th.\n",
    "* Using \"accuracy\" scoring by default, but can be changed by passing a different value to the \"scoring\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Random Forest model:\n",
    "res = cross_val_score(rf,X,y,cv=5)\n",
    "print(res)\n",
    "print(\"Average Validation Score: {}\".format(res.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Support Vector Machine model:\n",
    "res = cross_val_score(svm,X,y,cv=5)\n",
    "print(res)\n",
    "print(\"Average Validation Score: {}\".format(res.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #5:\n",
    "<span style=\"color:blue\">\n",
    "Let's use `cross_val_score` to evaluate quickly how a couple of other models will do at a high-level on this problem.\n",
    "* Let's evaluate `KNeighborsClassifier` (with parameter `n_neighbors`).\n",
    "* Also let's see how `LogisticRegression` model will do.\n",
    "</span>\n",
    "<div class=\"panel-group\" id=\"accordion-7\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-7\" href=\"#collapse1-7\">Hints</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-7\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "Similar code to the above, except that we should instantiate `KNeighborsClassifier(n_neighbors=?)` and `LogisticRegression()` models (one at a time, or in a loop).\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-7\" href=\"#collapse2-7\">Solution</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-7\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "One possible solution:\n",
    "<br><br>\n",
    "for (name, modl) in [ ('KNeighbors', KNeighborsClassifier(n_neighbors=11)), ('LogisticRegression', LogisticRegression()) ]:\n",
    "<p style=\"margin-left: 40px\">print(\"{}:\".format(name))</p>\n",
    "<p style=\"margin-left: 40px\">res = cross_val_score(modl,X,y,cv=5)</p>\n",
    "<p style=\"margin-left: 40px\">print(res)</p>\n",
    "<p style=\"margin-left: 40px\">print(\"Average Validation Score: {}\".format(res.mean()))</p>\n",
    "<p style=\"margin-left: 40px\">print()</p>\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNeighborsClassifier, LogisticRegression cross validation scores:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Grid Search to find optimal model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the \n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "params = {\n",
    "    'n_estimators' : [100, 150, 200],\n",
    "    'max_depth'    : [3, 5, 7],\n",
    "    'criterion'    : ('gini', 'entropy'),\n",
    "    'max_features' : [5, 7, 9]\n",
    "}\n",
    "\n",
    "gs_rf = GridSearchCV(model_rf, params, cv=5)\n",
    "gs_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_rf.best_estimator_)\n",
    "gs_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix:\n",
    "y_actual = y_test\n",
    "y_pred = gs_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_actual, y_pred)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_actual, y_pred)))\n",
    "print(\"Matthew's Corr Coef: {}\".format(matthews_corrcoef(y_actual, y_pred)))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_actual, y_pred),\n",
    "             columns=['Pred_Died','Pred_Survived'],\n",
    "             index=['Actual_Died','Actual_Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functionality of Random Forest classifier - provides feature importance ranking:\n",
    "pd.DataFrame({'importances': gs_rf.best_estimator_.feature_importances_ },index=X_train.columns).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly different importances for the un-optimized Random Forest Model:\n",
    "pd.DataFrame({'importances': rf.feature_importances_ },index=X_train.columns).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another grid-search example removing some of the correlated features:\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "params = {\n",
    "    'n_estimators' : [100, 150, 200],\n",
    "    'max_depth'    : [3, 5, 7],\n",
    "    'criterion'    : ('gini', 'entropy'),\n",
    "    'max_features' : [5, 7, 9]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(model_rf, params, cv=5)\n",
    "gs.fit(X_train.drop(['fareScaled','people_per_lastname'],axis=1),y_train)\n",
    "\n",
    "# Evaluation using Confusion Matrix and Evaluation Metrics:\n",
    "y_actual = y_test\n",
    "y_pred = gs.best_estimator_.predict(X_test.drop(['fareScaled','people_per_lastname'],axis=1))\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_actual, y_pred)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_actual, y_pred)))\n",
    "print(\"Matthew's Corr Coef: {}\".format(matthews_corrcoef(y_actual, y_pred)))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_actual, y_pred),\n",
    "             columns=['Pred_Died','Pred_Survived'],\n",
    "             index=['Actual_Died','Actual_Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #6:\n",
    "<span style=\"color:blue\">\n",
    "Let's do a grid search to find the optimal parameters for the SVC (Support Vector Classifier) model.\n",
    "<ul>\n",
    "    <li>Use Random Forest Grid Search as a guideline (from above).</li>\n",
    "    <li>Let's optimize the following parameters: C, kernel, degree.</li>\n",
    "    <li>See whether we can beat the evaluation metrics we got from the original model.</li>\n",
    "</ul>\n",
    "</span>\n",
    "<div class=\"panel-group\" id=\"accordion-6\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-6\" href=\"#collapse1-6\">Solution</a>\n",
    "    </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-6\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "Possible solution:<br>\n",
    "<br>\n",
    "model_svm = SVC(random_state=42)<br>\n",
    "params = {<br>\n",
    "    'C'            : [0.1, 1, 10],<br>\n",
    "    'kernel'       : ('rbf', 'poly'),<br>\n",
    "    'degree'       : [2, 3, 4]<br>\n",
    "}<br>\n",
    "<br>\n",
    "gs_svm = GridSearchCV(model_svm, params, cv=5)<br>\n",
    "gs_svm.fit(X_train,y_train)<br>\n",
    "<br>\n",
    "# Evaluation using Confusion Matrix and Evaluation Metrics:<br>\n",
    "y_actual = y_test<br>\n",
    "y_pred = gs_svm.best\\_estimator\\_.predict(X_test)<br>\n",
    "<br>\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_actual, y_pred)))<br>\n",
    "print(\"F1 Score: {}\".format(f1_score(y_actual, y_pred)))<br>\n",
    "print(\"Matthew's Corr Coef: {}\".format(matthews_corrcoef(y_actual, y_pred)))<br>\n",
    "<br>\n",
    "pd.DataFrame(confusion_matrix(y_actual, y_pred),<br>\n",
    "             columns=['Pred_Died','Pred_Survived'],<br>\n",
    "             index=['Actual_Died','Actual_Survived'])\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Grid Search:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Submission Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blind = combined_df[combined_df.Survived.isnull()].drop(['Survived'],axis=1)\n",
    "y_blind = gs_rf.best_estimator_.predict(X_blind,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'PassengerId' : X_blind.index.values+1, 'Survived' : y_blind.astype(int)}).to_csv('rf_submission_gs1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls rf_submission_gs1.csv\n",
    "! head -5 rf_submission_gs1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Public Leaderboard Score: 0.78947 (~ 3% jump from gender-only submission)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
